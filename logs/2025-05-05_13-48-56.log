[[36m2025-05-05 13:48:56[0m][[94mGalLoP[0m][[32mINFO[0m] - Setting the seed to 1[0m
[[36m2025-05-05 13:48:56[0m][[94mGalLoP[0m][[32mINFO[0m] - Running experiment imagenet_shot_16[0m
[[36m2025-05-05 13:49:00[0m][[94mGalLoP[0m][[32mINFO[0m] - Using template: A photo of a <class_name>[0m
[[36m2025-05-05 13:49:10[0m][[94mGalLoP[0m][[33mWARNING[0m] - Missing keys in CLIP: ['local_prompts', 'vit_predictor.vit.embeddings.cls_token', 'vit_predictor.vit.embeddings.position_embeddings', 'vit_predictor.vit.embeddings.patch_embeddings.projection.weight', 'vit_predictor.vit.embeddings.patch_embeddings.projection.bias', 'vit_predictor.vit.encoder.layer.0.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.0.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.0.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.0.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.0.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.0.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.0.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.0.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.0.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.0.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.0.output.dense.weight', 'vit_predictor.vit.encoder.layer.0.output.dense.bias', 'vit_predictor.vit.encoder.layer.0.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.0.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.0.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.0.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.1.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.1.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.1.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.1.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.1.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.1.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.1.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.1.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.1.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.1.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.1.output.dense.weight', 'vit_predictor.vit.encoder.layer.1.output.dense.bias', 'vit_predictor.vit.encoder.layer.1.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.1.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.1.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.1.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.2.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.2.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.2.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.2.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.2.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.2.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.2.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.2.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.2.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.2.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.2.output.dense.weight', 'vit_predictor.vit.encoder.layer.2.output.dense.bias', 'vit_predictor.vit.encoder.layer.2.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.2.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.2.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.2.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.3.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.3.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.3.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.3.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.3.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.3.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.3.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.3.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.3.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.3.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.3.output.dense.weight', 'vit_predictor.vit.encoder.layer.3.output.dense.bias', 'vit_predictor.vit.encoder.layer.3.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.3.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.3.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.3.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.4.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.4.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.4.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.4.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.4.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.4.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.4.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.4.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.4.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.4.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.4.output.dense.weight', 'vit_predictor.vit.encoder.layer.4.output.dense.bias', 'vit_predictor.vit.encoder.layer.4.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.4.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.4.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.4.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.5.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.5.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.5.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.5.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.5.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.5.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.5.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.5.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.5.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.5.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.5.output.dense.weight', 'vit_predictor.vit.encoder.layer.5.output.dense.bias', 'vit_predictor.vit.encoder.layer.5.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.5.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.5.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.5.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.6.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.6.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.6.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.6.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.6.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.6.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.6.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.6.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.6.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.6.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.6.output.dense.weight', 'vit_predictor.vit.encoder.layer.6.output.dense.bias', 'vit_predictor.vit.encoder.layer.6.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.6.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.6.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.6.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.7.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.7.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.7.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.7.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.7.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.7.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.7.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.7.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.7.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.7.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.7.output.dense.weight', 'vit_predictor.vit.encoder.layer.7.output.dense.bias', 'vit_predictor.vit.encoder.layer.7.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.7.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.7.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.7.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.8.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.8.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.8.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.8.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.8.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.8.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.8.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.8.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.8.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.8.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.8.output.dense.weight', 'vit_predictor.vit.encoder.layer.8.output.dense.bias', 'vit_predictor.vit.encoder.layer.8.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.8.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.8.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.8.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.9.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.9.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.9.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.9.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.9.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.9.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.9.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.9.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.9.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.9.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.9.output.dense.weight', 'vit_predictor.vit.encoder.layer.9.output.dense.bias', 'vit_predictor.vit.encoder.layer.9.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.9.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.9.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.9.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.10.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.10.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.10.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.10.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.10.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.10.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.10.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.10.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.10.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.10.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.10.output.dense.weight', 'vit_predictor.vit.encoder.layer.10.output.dense.bias', 'vit_predictor.vit.encoder.layer.10.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.10.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.10.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.10.layernorm_after.bias', 'vit_predictor.vit.encoder.layer.11.attention.attention.query.weight', 'vit_predictor.vit.encoder.layer.11.attention.attention.query.bias', 'vit_predictor.vit.encoder.layer.11.attention.attention.key.weight', 'vit_predictor.vit.encoder.layer.11.attention.attention.key.bias', 'vit_predictor.vit.encoder.layer.11.attention.attention.value.weight', 'vit_predictor.vit.encoder.layer.11.attention.attention.value.bias', 'vit_predictor.vit.encoder.layer.11.attention.output.dense.weight', 'vit_predictor.vit.encoder.layer.11.attention.output.dense.bias', 'vit_predictor.vit.encoder.layer.11.intermediate.dense.weight', 'vit_predictor.vit.encoder.layer.11.intermediate.dense.bias', 'vit_predictor.vit.encoder.layer.11.output.dense.weight', 'vit_predictor.vit.encoder.layer.11.output.dense.bias', 'vit_predictor.vit.encoder.layer.11.layernorm_before.weight', 'vit_predictor.vit.encoder.layer.11.layernorm_before.bias', 'vit_predictor.vit.encoder.layer.11.layernorm_after.weight', 'vit_predictor.vit.encoder.layer.11.layernorm_after.bias', 'vit_predictor.vit.layernorm.weight', 'vit_predictor.vit.layernorm.bias', 'vit_predictor.classifier.weight', 'vit_predictor.classifier.bias', 'local_proj.linear.weight'][0m
[[36m2025-05-05 13:49:10[0m][[94mGalLoP[0m][[32mINFO[0m] - local_prompts added to params_group[0m
[[36m2025-05-05 13:49:10[0m][[94mGalLoP[0m][[32mINFO[0m] - local_proj.linear.weight added to params_group[0m
[[36m2025-05-05 13:56:13[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.915 top1: 78.231 top1_global: 79.069 top1_local: 31.919[0m
[[36m2025-05-05 14:02:59[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.887 top1: 78.850 top1_global: 78.369 top1_local: 31.456[0m
[[36m2025-05-05 14:09:46[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.885 top1: 79.325 top1_global: 78.737 top1_local: 30.981[0m
[[36m2025-05-05 14:16:48[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.875 top1: 79.412 top1_global: 78.662 top1_local: 30.125[0m
[[36m2025-05-05 14:23:44[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.858 top1: 79.862 top1_global: 79.175 top1_local: 28.756[0m
[[36m2025-05-05 14:30:30[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.852 top1: 79.706 top1_global: 79.088 top1_local: 28.212[0m
[[36m2025-05-05 14:37:17[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.865 top1: 79.688 top1_global: 79.069 top1_local: 27.812[0m
[[36m2025-05-05 14:44:04[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.859 top1: 79.950 top1_global: 79.050 top1_local: 27.262[0m
[[36m2025-05-05 14:50:50[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.862 top1: 79.294 top1_global: 78.537 top1_local: 26.175[0m
[[36m2025-05-05 14:57:36[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.861 top1: 79.537 top1_global: 78.638 top1_local: 22.425[0m
[[36m2025-05-05 15:04:23[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.857 top1: 79.769 top1_global: 78.763 top1_local: 24.369[0m
[[36m2025-05-05 15:04:23[0m][[94mGalLoP[0m][[32mINFO[0m] - Evaluation[0m
[[36m2025-05-05 15:05:40[0m][[94mGalLoP[0m][[32mINFO[0m] - Evaluation metrics:  * top1: 79.780 top1_global: 78.500 top1_local: 25.882[0m
[[36m2025-05-05 15:12:07[0m][[94mGalLoP[0m][[32mINFO[0m] - OOD Evaluation metrics with temperature scale 1.0 (FPR95 / AUROC): [0m
[[36m2025-05-05 15:12:07[0m][[94mGalLoP[0m][[32mINFO[0m] - *** =============[0m
[[36m2025-05-05 15:12:07[0m][[94mGalLoP[0m][[32mINFO[0m] - *           Ninco          SSB-Hard       iNaturalist               SUN            Places          Textures         OpenImage           Average[0m
[[36m2025-05-05 15:12:07[0m][[94mGalLoP[0m][[32mINFO[0m] - *   48.435 / 89.593&  64.792 / 82.989&  12.650 / 97.625&  44.590 / 90.996&  50.550 / 89.036&  44.415 / 89.327&  25.533 / 95.399&   41.566 / 90.709[0m
[[36m2025-05-05 15:18:54[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.833 top1: 79.825 top1_global: 78.787 top1_local: 24.562[0m
[[36m2025-05-05 15:25:41[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.840 top1: 79.981 top1_global: 78.900 top1_local: 25.506[0m
[[36m2025-05-05 15:32:28[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.843 top1: 79.531 top1_global: 78.425 top1_local: 24.944[0m
[[36m2025-05-05 15:39:17[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.824 top1: 80.194 top1_global: 78.925 top1_local: 23.238[0m
[[36m2025-05-05 15:46:03[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.844 top1: 79.575 top1_global: 78.662 top1_local: 23.906[0m
[[36m2025-05-05 15:52:50[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.845 top1: 79.581 top1_global: 78.744 top1_local: 23.906[0m
[[36m2025-05-05 15:59:38[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.826 top1: 80.219 top1_global: 78.612 top1_local: 25.125[0m
[[36m2025-05-05 16:06:35[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.824 top1: 80.125 top1_global: 79.344 top1_local: 23.750[0m
[[36m2025-05-05 16:13:23[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.829 top1: 80.025 top1_global: 78.581 top1_local: 24.156[0m
[[36m2025-05-05 16:20:10[0m][[94mGalLoP[0m][[32mINFO[0m] -  * loss: 0.821 top1: 80.019 top1_global: 78.800 top1_local: 24.506[0m
[[36m2025-05-05 16:20:10[0m][[94mGalLoP[0m][[32mINFO[0m] - Evaluation[0m
[[36m2025-05-05 16:22:02[0m][[94mGalLoP[0m][[32mINFO[0m] - Evaluation metrics:  * top1: 79.147 top1_global: 78.500 top1_local: 26.398[0m
[[36m2025-05-05 16:32:39[0m][[94mGalLoP[0m][[32mINFO[0m] - OOD Evaluation metrics with temperature scale 1.0 (FPR95 / AUROC): [0m
[[36m2025-05-05 16:32:39[0m][[94mGalLoP[0m][[32mINFO[0m] - *** =============[0m
[[36m2025-05-05 16:32:39[0m][[94mGalLoP[0m][[32mINFO[0m] - *           Ninco          SSB-Hard       iNaturalist               SUN            Places          Textures         OpenImage           Average[0m
[[36m2025-05-05 16:32:39[0m][[94mGalLoP[0m][[32mINFO[0m] - *   48.350 / 89.562&  64.892 / 82.924&  12.620 / 97.636&  44.420 / 91.031&  50.560 / 89.050&  44.397 / 89.302&  25.499 / 95.404&   41.534 / 90.701[0m
